# -*- coding: utf-8 -*-
"""audio2art.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12lnLFQm8Fr_XlaSBrBQoDoUMbRGLhtr3
"""

# Install Python packages
!pip install streamlit diffusers accelerate transformers torch librosa --quiet

# Install Node.js and LocalTunnel
!apt install nodejs npm -y
!npm install -g localtunnel

# Commented out IPython magic to ensure Python compatibility.
# %%writefile ImageModel.py
# import librosa as lb
# import torch
# from typing import Literal
# from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC
# from diffusers import StableDiffusionPipeline
# import time
# 
# def promptgen(file):
#     tokenizer = Wav2Vec2Tokenizer.from_pretrained('facebook/wav2vec2-base-960h')
#     model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')
#     waveform, rate = lb.load(file, sr=16000)
#     input_values = tokenizer(waveform, return_tensors='pt').input_values
#     logits = model(input_values).logits
#     predicted_ids = torch.argmax(logits, dim=-1)
#     transcription = tokenizer.batch_decode(predicted_ids)
#     return transcription[0]
# 
# def text2image(prompt: str, repo_id: Literal["CompVis/stable-diffusion-v1-4", "stabilityai/stable-diffusion-2-1"]):
#     seed = 2024
#     generator = torch.manual_seed(seed)
#     NUM_INFERENCE_STEPS = 15
#     start = time.time()
# 
#     if torch.cuda.is_available():
#         pipe = StableDiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
#         pipe = pipe.to("cuda")
#     else:
#         pipe = StableDiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float32)
# 
#     image = pipe(prompt=prompt, num_inference_steps=NUM_INFERENCE_STEPS, generator=generator).images[0]
#     end = time.time()
#     return image, start, end
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from ImageModel import promptgen, text2image
# from io import BytesIO
# 
# def app():
#     st.title("ðŸŽ¤ Audio2Art: Turn Audio Prompts Into Images")
# 
#     upload_file = st.file_uploader("ðŸŽµ Upload your .wav file", type=["wav"])
# 
#     model = st.selectbox("Choose a Stable Diffusion model", [
#         "CompVis/stable-diffusion-v1-4",
#         "stabilityai/stable-diffusion-2-1"
#     ])
# 
#     with st.form("my_form"):
#         submit = st.form_submit_button(label="âœ¨ Generate Image")
# 
#         if submit and upload_file:
#             with st.spinner("Transcribing and generating... Please wait."):
#                 prompt = promptgen(upload_file)
#                 st.markdown(f"**ðŸŽ¯ Prompt from Audio:** `{prompt}`")
#                 im, start, end = text2image(prompt, model)
# 
#                 buf = BytesIO()
#                 im.save(buf, format="PNG")
#                 byte_im = buf.getvalue()
# 
#                 st.success(f"âœ… Image generated in {round(end - start, 2)} sec")
#                 st.image(im)
# 
#                 st.download_button("ðŸ“¥ Download Image", data=byte_im, file_name="image.png", mime="image/png")
# 
# if __name__ == "__main__":
#     app()
#

!streamlit run app.py &>/content/logs.txt &

# Sleep for 10 sec to wait for app to launch
import time
time.sleep(10)

# Launch localtunnel without prompting password
!npx localtunnel --port 8501 --subdomain audio2art-demo

!curl https://loca.lt/mytunnelpassword
